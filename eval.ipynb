{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422d4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup functions for the demo\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import importlib\n",
    "import yaml\n",
    "import warnings\n",
    "import albumentations\n",
    "import glob\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from omegaconf import OmegaConf\n",
    "from skimage.io import imread, imsave\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.notebook import tqdm\n",
    "from core.modules.util import box_mask, BatchRandomMask\n",
    "from core.modules.losses.lpips import LPIPS\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def imshow(images, titles=None, save_path=None):\n",
    "    n_img = len(images)\n",
    "    plt.rcParams['figure.figsize'] = [4*n_img, 4*n_img]\n",
    "    \n",
    "    if n_img > 1:\n",
    "        fig, ax = plt.subplots(1, n_img)\n",
    "        for i in range(n_img):\n",
    "            if titles is not None and i < len(titles):\n",
    "                ax[i].set_title(titles[i])\n",
    "            ax[i].axis('off')\n",
    "            ax[i].imshow(images[i])\n",
    "    else:\n",
    "        if titles is not None:\n",
    "            plt.set_titile(titles[0])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[0])\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "def center_crop(image, s=512):\n",
    "    h, w = image.shape[:2]\n",
    "    if s > h or s > w:\n",
    "        image = rescale(image, s/min(h,w), anti_aliasing=True)\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        h, w = image.shape[:2]\n",
    "    ih = (h - s) // 2   \n",
    "    iw = (w - s) // 2\n",
    "    return image[ih:ih+s, iw:iw+s]\n",
    "    \n",
    "# load a single input\n",
    "def preprocess(x, res=256, normalize=True):\n",
    "    if normalize:\n",
    "        x = x.transpose(2,0,1)\n",
    "        x = (torch.from_numpy(x).float().to(device) / 127.5 - 1).unsqueeze(0)\n",
    "    else:\n",
    "        x = torch.from_numpy(x).float().to(device).unsqueeze(0)\n",
    "    return torch.nn.functional.interpolate(x, size=(res,res))\n",
    "    \n",
    "def to_img(x):\n",
    "    x = (x.permute(0, 2, 3, 1) * 127.5 + 127.5).round().clamp(0, 255).to(torch.uint8)\n",
    "    return x[0].detach().cpu().numpy()\n",
    "\n",
    "def readmask(path):\n",
    "    mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
    "    return mask\n",
    "\n",
    "def read_image(path):\n",
    "    image = center_crop(imread(path))\n",
    "    image = preprocess(image, res=target_res)\n",
    "    return image[:,:3]\n",
    "\n",
    "def get_data(k, res, p=None):\n",
    "    if p is None:\n",
    "        p = places_val_files[k]                 \n",
    "    gt = center_crop(imread(p))\n",
    "    if len(gt.shape) == 2:\n",
    "        gt = np.repeat(gt[...,None], 3, axis=2) \n",
    "    gt = preprocess(gt, res)\n",
    "    try:\n",
    "        mask_in = preprocess(readmask(os.path.join(maskfolder, f\"{ids[k]}.png\"))[None], res,normalize=False)\n",
    "    except Exception:\n",
    "        return gt, None\n",
    "    return gt, mask_in\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6747fc-822f-49a7-b493-7398697304c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "target_model = \"places\" # model: places | celeba\n",
    "target_res = 256 # image resolution: 256x256/512x512 for Places365, 256x256 for CelebA\n",
    "gpu_idx = input(\"specify the gpu index for loading the model (e.g. 0): \") # gpu index for loading the model\n",
    "\n",
    "# Model Parameters\n",
    "CLAMP_RATIO = 0.25 # clamp ratio for the restrictive encoder\n",
    "SAMPLING_RATIO = 0.2 # sampling ratio for iterative token predictions\n",
    "TEMPERATURE = 1.0 # temperature scaling before logits softmax\n",
    "TEMPERATURE_ANNEALING = 0.9 # annealing factor applied to the temperature in each iteration\n",
    "DETERMINISTIC = False # whether we want the model to generate deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f854e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Restored from ckpts/places256_vqgan1024_BASE.ckpt\n",
      "Restored from ckpts/places256_partialencoder.ckpt\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "current_dir = os.path.abspath('.')\n",
    "\n",
    "if target_model == \"places\":\n",
    "    config_path = \"configs/places_inpainting_512.yaml\" if target_res == 512 else \"configs/places_inpainting.yaml\"\n",
    "else:\n",
    "    assert target_model == 'celeba'\n",
    "    target_res = 256\n",
    "    config_path = \"configs/celeba_inpainting.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "config['data']['params']['batch_size'] = 1\n",
    "model = instantiate_from_config(config.model).to(device)\n",
    "\n",
    "# Load the model's state dictionary\n",
    "sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd, strict=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# Check CUDA availability and print memory summary\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(torch.cuda.memory_summary(device=device))\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU.\")\n",
    "\n",
    "# Optional: Verify model loading and device allocation\n",
    "print(f\"Model loaded on device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4dd07b7-62a0-4b01-b8eb-8fb029306878",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load an example input (at the moment we only have some examples from Places365-Standard)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Images\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m input_image_list \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_data/*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m read_image(input_image_list[idx])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "# Load an example input (at the moment we only have some examples from Places365-Standard)\n",
    "\n",
    "# Images\n",
    "input_image_list = glob.glob(\"example_data/*.png\")\n",
    "idx = 0 \n",
    "x = read_image(input_image_list[idx]).to(device)\n",
    "if x.shape[-1] != 256:\n",
    "    x_down = torch.nn.functional.interpolate(x, (256, 256))\n",
    "else:\n",
    "    x_down = x\n",
    "\n",
    "# Mask\n",
    "mask = box_mask(x_down.shape, device, 0.3, det=True).float() # box mask\n",
    "# mask = torch.from_numpy(BatchRandomMask(x_down.shape[0], x_down.shape[-1], hole_range=[0.0,1.0])).to(device) # free-form mask\n",
    "if target_res != mask.shape[-1]:\n",
    "    mask_up = torch.nn.functional.interpolate(mask, (target_res, target_res))\n",
    "else:\n",
    "    mask_up = mask\n",
    "\n",
    "imshow([to_img(x*mask_up)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39449184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     17\u001b[0m     index_sample, probs, candidates \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msample(z_start_indices\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice), \n\u001b[1;32m     18\u001b[0m                                c_indices\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[1;32m     19\u001b[0m                                sampling_ratio\u001b[38;5;241m=\u001b[39mSAMPLING_RATIO,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m                               )\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index_sample, probs, candidates\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     30\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(mask)    \n\u001b[1;32m     31\u001b[0m     VQModel, Encoder, Transformer, Unet \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhelper_model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Inference code\n",
    "\n",
    "'''\n",
    "candidate settings : \n",
    "1) 0.33 ratio, 0.8 degrad, 1.0t, no topk (more error prone)\n",
    "2) 0.1 ratio, 0.95 degrad. 1.0t, no topk\n",
    "3) 0.2 ratio, 0.9 degrad, 1.0t, no topk (default)\n",
    "'''\n",
    "def forward_to_indices(model, batch, z_indices, mask):\n",
    "    x, c = model.get_xc(batch)\n",
    "    x = x.to(device=device).float()\n",
    "    c = c.to(device=device).float()\n",
    "    quant_c, c_indices = model.encode_to_c(c)\n",
    "    mask = model.preprocess_mask(mask, z_indices)\n",
    "    r_indices = torch.full_like(z_indices, model.mask_token)\n",
    "    z_start_indices = mask*z_indices+(1-mask)*r_indices      \n",
    "    index_sample, probs, candidates = model.sample(z_start_indices.to(device=device), \n",
    "                               c_indices.to(device=device),\n",
    "                               sampling_ratio=SAMPLING_RATIO,\n",
    "                               temperature=TEMPERATURE,\n",
    "                               sample=not DETERMINISTIC,\n",
    "                               temperature_degradation=TEMPERATURE_ANNEALING,\n",
    "                               top_k=None,\n",
    "                               return_probs=True,\n",
    "                               scheduler='cosine',\n",
    "                              )\n",
    "    return index_sample, probs, candidates\n",
    "\n",
    "with torch.no_grad():\n",
    "    mask = torch.round(mask)    \n",
    "    VQModel, Encoder, Transformer, Unet = model.helper_model\n",
    "    VQModel = VQModel.to(device)\n",
    "    Encoder = Encoder.to(device)\n",
    "    Transformer = Transformer.to(device)        \n",
    "    quant_z, _, info, mask_out = Encoder.encode(x_down*mask, mask, clamp_ratio=CLAMP_RATIO)\n",
    "    mask_out = mask_out.reshape(x.shape[0], -1)\n",
    "    z_indices = info[2].reshape(x.shape[0], -1)\n",
    "    \n",
    "    # batch input is always 256x256\n",
    "    new_batch = {'image': (x_down*mask).permute(0,2,3,1)}\n",
    "    \n",
    "    z_indices_complete, probs, candidates = forward_to_indices(Transformer, new_batch, z_indices, mask_out)\n",
    "    B, C, H, W = quant_z.shape\n",
    "    quant_z_complete = VQModel.quantize.get_codebook_entry(z_indices_complete.reshape(-1).int(), shape=(B, H, W, C))   \n",
    "    dec, _, mout, f0, f1 = model.current_model(new_batch, \n",
    "                                quant=quant_z_complete, \n",
    "                                mask_in=mask, \n",
    "                                mask_out=mask_out.reshape(B, 1, H, W),\n",
    "                                return_fstg=False, debug=True)  \n",
    "    rec = x_down * mask + dec * (1-mask) \n",
    "    if Unet is not None:\n",
    "        Unet = Unet.to(device)\n",
    "        rec = Unet.refine(rec, None, recomp=False)\n",
    "        rec = x * mask_up + rec * (1-mask_up)\n",
    "\n",
    "imshow([to_img(x*mask_up),to_img(rec)], titles=['input', 'prediction'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
